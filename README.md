# 100DaysOfMLCode
Here's the progress for #100DaysOfMLCode Challenge.

**Find my Blog [here](https://www.tanaytoshniwal.com)**

**[Link to my website](https://www.tanaytoshniwal.com)**

## Day 0 (**22<sup>nd</sup> July 2018**):
---
### Progress
+   Collection of Study Material
+   Setup the Programming Environment, i.e, installing dependencies

### Thoughts
Really Excited to learn Machine Learning in a more effective way

## Day 1 (**23<sup>rd</sup> July 2018**):
---
### Progress
+   Learnt about Linear and Logistic Regression
+   Collected Dataset from Kaggle about Breast Cancer
+   Modified the dataset to make use of it and understood the structure of a dataset
+   Trained a logistic regression classifier to predict Breast Cancer to be Maligant or Benign using Naive Bayes
+   Used Scikit Learn Library for Logistic Regression

### Thoughts
It was full of fun, learnt a lot about dealing with **rank 1 arrays** as well as keeping a track on the shape of the matrices.

## Day 2 (**24<sup>th</sup> July 2018**):
---
### Progress
+   Learnt about Vectorization in detail and its importance in Machine Learning practices
+   Completed the Machine Learning Crash Course till **Generalization**

### Thoughts
Now I realized why matrix manipulation is important. Vectorization actually helps a lot and it is the key feature because of which now we are able to run Machine Learning Algorithms much more faster.

## Day 3 (**25<sup>th</sup> July 2018**):
---
### Progress
+   Continued my Deep Learning Specialization on Coursera
+   Learnt more about Gradient Descent and how it works
+   Studied various methods of regularizing the Neural Network to prevent overfitting

### Thoughts
Gradient Descent is a great algorithm in Machine Learning. Enjoyed a lot while learning about this algorithm and the methods to prevent overfitting.

## Day 4 (**26<sup>th</sup> July 2018**):
---
### Progress
+   Learnt about Mini-batch Gradient Descent and a bit about Stochastic Gradient Descent
+   Made some progress with Machine Learning Crash Course.

### Thoughts
It's actually great to make progress before looking through the entire training set by using small batches and performing Gradient Descent on each such batch.

## Day 5 (**27<sup>th</sup> July 2018**):
---
### Progress
+   Implemented a Linear Regression Model on the Boston Dataset.
+   Used Scikit Learn Library for the dataset and the Linear Regression Model.
+   Used Matplot Lib to plot the predictions.

### Thoughts
It was challenging to train the Linear Regression Model on a dataset I downloaded from Kaggle, so ended up using the Boston Dataset.

## Day 6 (**28<sup>th</sup> July 2018**):
---
### Progress
+   Made some progress with the Deep Learning Specialization on Coursera

### Thoughts
Not much for the day but got to learn new things.

## Day 7 (**29<sup>th</sup> July 2018**):
---
### Progress
+   Made some with the ML Crash Course by Google.
+   Learnt about Exponentially Weighted Averages and the Bias Correction in Exponentially Weighted Averages.

### Thoughts
Got to learn some new Optimization Algorithms

## Day 8 (**30<sup>th</sup> July 2018**):
---
### Progress
+   Learnt about Adam Optimization Algorithm
+   Implemented Adam Optimization Algorithm with the Deep Learning Course Assignments

### Thoughts
Haven't thought of any other Optimization Algorithm to me more efficient as compared to Gradient Descent.

## Day 9 (**31<sup>st</sup> July 2018**):
---
### Progress
+   Learnt and revised about basics of Building a Neural Network, its working, forward prop and backward prop
+   Made some progress with ML Crash Course

### Thoughts
Thinking of getting started with Tensorflow for Machine Learning and Deep Learning Applications