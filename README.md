# 100DaysOfMLCode
Here's the progress for #100DaysOfMLCode Challenge.

**Find my Blog [here](https://www.tanaytoshniwal.com)**

**[Link to my website](https://www.tanaytoshniwal.com)**

## Day 0 (**22<sup>nd</sup> July 2018**):
---
### Progress
+   Collection of Study Material
+   Setup the Programming Environment, i.e, installing dependencies

### Thoughts
Really Excited to learn Machine Learning in a more effective way

## Day 1 (**23<sup>rd</sup> July 2018**):
---
### Progress
+   Learnt about Linear and Logistic Regression
+   Collected Dataset from Kaggle about Breast Cancer
+   Modified the dataset to make use of it and understood the structure of a dataset
+   Trained a logistic regression classifier to predict Breast Cancer to be Maligant or Benign using Naive Bayes
+   Used Scikit Learn Library for Logistic Regression

### Thoughts
It was full of fun, learnt a lot about dealing with **rank 1 arrays** as well as keeping a track on the shape of the matrices.

## Day 2 (**24<sup>th</sup> July 2018**):
---
### Progress
+   Learnt about Vectorization in detail and its importance in Machine Learning practices
+   Completed the Machine Learning Crash Course till **Generalization**

### Thoughts
Now I realized why matrix manipulation is important. Vectorization actually helps a lot and it is the key feature because of which now we are able to run Machine Learning Algorithms much more faster.

## Day 3 (**25<sup>th</sup> July 2018**):
---
### Progress
+   Continued my Deep Learning Specialization on Coursera
+   Learnt more about Gradient Descent and how it works
+   Studied various methods of regularizing the Neural Network to prevent overfitting

### Thoughts
Gradient Descent is a great algorithm in Machine Learning. Enjoyed a lot while learning about this algorithm and the methods to prevent overfitting.

## Day 4 (**26<sup>th</sup> July 2018**):
---
### Progress
+   Learnt about Mini-batch Gradient Descent and a bit about Stochastic Gradient Descent
+   Made some progress with Machine Learning Crash Course.

### Thoughts
It's actually great to make progress before looking through the entire training set by using small batches and performing Gradient Descent on each such batch.

## Day 5 (**27<sup>th</sup> July 2018**):
---
### Progress
+   Implemented a Linear Regression Model on the Boston Dataset.
+   Used Scikit Learn Library for the dataset and the Linear Regression Model.
+   Used Matplot Lib to plot the predictions.

### Thoughts
It was challenging to train the Linear Regression Model on a dataset I downloaded from Kaggle, so ended up using the Boston Dataset.

## Day 6 (**28<sup>th</sup> July 2018**):
---
### Progress
+   Made some progress with the Deep Learning Specialization on Coursera

### Thoughts
Not much for the day but got to learn new things.

## Day 7 (**29<sup>th</sup> July 2018**):
---
### Progress
+   Made some with the ML Crash Course by Google.
+   Learnt about Exponentially Weighted Averages and the Bias Correction in Exponentially Weighted Averages.

### Thoughts
Got to learn some new Optimization Algorithms

## Day 8 (**30<sup>th</sup> July 2018**):
---
### Progress
+   Learnt about Adam Optimization Algorithm
+   Implemented Adam Optimization Algorithm with the Deep Learning Course Assignments

### Thoughts
Haven't thought of any other Optimization Algorithm to me more efficient as compared to Gradient Descent.

## Day 9 (**31<sup>st</sup> July 2018**):
---
### Progress
+   Learnt and revised about basics of Building a Neural Network, its working, forward prop and backward prop
+   Made some progress with ML Crash Course

### Thoughts
Thinking of getting started with Tensorflow for Machine Learning and Deep Learning Applications

## Day 10 (**1<sup>st</sup> August 2018**):
---
### Progress
+   Implemented Deep Dream on Live Video Stream using OpenCV

### Thoughts
It was bit challenging, still the video stream lags

## Day 11 (**2<sup>nd</sup> August 2018**):
---
### Progress
+   Learnt about Batch Normalization and its working
+	Made some progress with the Deep Learning Specialization on Coursera

### Thoughts
Batch Normalization is a great technique to normalize the activations in a Neural Network. Enjoyed learning about its working

## Day 12 (**3<sup>rd</sup> August 2018**):
---
### Progress
+   Started with TensorFlow Framework
+   Learned about basics of Tensors and how to run a session in tensorflow

### Thoughts
Machine Learning Frameworks are cool. Only one line of code reduces too much of code and complexity.

## Day 13 (**4<sup>th</sup> August 2018**):
---
### Progress
+   Completed the Second Course of Deep Learning Specialization on Coursera(Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization)

### Thoughts
Really Enjoyed a lot during the course and learnt a lot about tuning Hyperparameters, various Optimization Algorithms and much more.

## Day 14 (**5<sup>th</sup> August 2018**):
---
### Progress
+   Started with the Course 3 of Deep Learning Specialization

### Thoughts
Not much for the day

## Day 21 (**12<sup>th</sup> August 2018**):
---
### Progress
+   Completed the third Course of Deep Learning Specialization on Coursera(Structuring Machine Learning Projects)


### Thoughts
Python is a great programming language. It is a bit difficult to switch to python from java but its worthy to do so.

## Day 29 (**20<sup>th</sup> August 2018**):
---
### Progress
+   Made some progress with Machine Learning Crash Course
+   Saw some videos on Machine Learning with Python

### Thoughts
Not much for the week (PS: University Exams wont let you do something Interesting and Beneficial).

